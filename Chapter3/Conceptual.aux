\relax 
\providecommand*\new@tpo@label[2]{}
\providecommand\hyper@newdestlabel[2]{}
\providecommand*\HyPL@Entry[1]{}
\HyPL@Entry{0<</S/D>>}
\@writefile{toc}{\contentsline {subsection}{\nonumberline Import notebook funcs}{1}{subsection*.2}\protected@file@percent }
\newlabel{import-notebook-funcs}{{}{1}{Import notebook funcs}{subsection*.2}{}}
\@writefile{toc}{\contentsline {subsection}{\nonumberline Import user funcs}{1}{subsection*.4}\protected@file@percent }
\newlabel{import-user-funcs}{{}{1}{Import user funcs}{subsection*.4}{}}
\@writefile{toc}{\contentsline {subsection}{\nonumberline Import libraries}{1}{subsection*.6}\protected@file@percent }
\newlabel{import-libraries}{{}{1}{Import libraries}{subsection*.6}{}}
\@writefile{toc}{\contentsline {subsection}{\nonumberline 1. Describe the null hypotheses to which the p-values given in Table 3.4 correspond. Explain what conclusions you can draw based on these p-values. Your explanation should be phrased in terms of sales, TV, radio, and newspaper, rather than in terms of the coefficients of the linear model.}{1}{subsection*.8}\protected@file@percent }
\newlabel{describe-the-null-hypotheses-to-which-the-p-values-given-in-table-3.4-correspond.-explain-what-conclusions-you-can-draw-based-on-these-p-values.-your-explanation-should-be-phrased-in-terms-of-sales-tv-radio-and-newspaper-rather-than-in-terms-of-the-coefficients-of-the-linear-model.}{{}{1}{1. Describe the null hypotheses to which the p-values given in Table 3.4 correspond. Explain what conclusions you can draw based on these p-values. Your explanation should be phrased in terms of sales, TV, radio, and newspaper, rather than in terms of the coefficients of the linear model}{subsection*.8}{}}
\gdef \LT@i {\LT@entry 
    {3}{58.40671pt}\LT@entry 
    {1}{62.4795pt}\LT@entry 
    {1}{85.31024pt}\LT@entry 
    {1}{57.73816pt}\LT@entry 
    {3}{48.58456pt}}
\@writefile{toc}{\contentsline {subsection}{\nonumberline 2. Carefully explain the differences between the KNN classifier and KNN regression methods.}{2}{subsection*.10}\protected@file@percent }
\newlabel{carefully-explain-the-differences-between-the-knn-classifier-and-knn-regression-methods.}{{}{2}{2. Carefully explain the differences between the KNN classifier and KNN regression methods}{subsection*.10}{}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline KNN Classifier}{2}{subsubsection*.12}\protected@file@percent }
\newlabel{knn-classifier}{{}{2}{KNN Classifier}{subsubsection*.12}{}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline KNN Regression}{2}{subsubsection*.14}\protected@file@percent }
\newlabel{knn-regression}{{}{2}{KNN Regression}{subsubsection*.14}{}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline \emph  {Similarities:}}{2}{paragraph*.16}\protected@file@percent }
\newlabel{similarities}{{}{2}{\texorpdfstring {\emph {Similarities:}}{Similarities:}}{paragraph*.16}{}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline \emph  {Differences:}}{3}{paragraph*.18}\protected@file@percent }
\newlabel{differences}{{}{3}{\texorpdfstring {\emph {Differences:}}{Differences:}}{paragraph*.18}{}}
\@writefile{toc}{\contentsline {subparagraph}{\nonumberline \emph  {Classification:}}{3}{subparagraph*.20}\protected@file@percent }
\newlabel{classification}{{}{3}{\texorpdfstring {\emph {Classification:}}{Classification:}}{subparagraph*.20}{}}
\@writefile{toc}{\contentsline {subparagraph}{\nonumberline \emph  {Regression:}}{3}{subparagraph*.22}\protected@file@percent }
\newlabel{regression}{{}{3}{\texorpdfstring {\emph {Regression:}}{Regression:}}{subparagraph*.22}{}}
\@writefile{toc}{\contentsline {subparagraph}{\nonumberline \emph  {Key differences:}}{3}{subparagraph*.24}\protected@file@percent }
\newlabel{key-differences}{{}{3}{\texorpdfstring {\emph {Key differences:}}{Key differences:}}{subparagraph*.24}{}}
\@writefile{toc}{\contentsline {subparagraph}{\nonumberline \emph  {KNN Classification:}}{3}{subparagraph*.26}\protected@file@percent }
\newlabel{knn-classification}{{}{3}{\texorpdfstring {\emph {KNN Classification:}}{KNN Classification:}}{subparagraph*.26}{}}
\@writefile{toc}{\contentsline {subparagraph}{\nonumberline \emph  {KNN Regression:}}{3}{subparagraph*.28}\protected@file@percent }
\newlabel{knn-regression-1}{{}{3}{\texorpdfstring {\emph {KNN Regression:}}{KNN Regression:}}{subparagraph*.28}{}}
\@writefile{toc}{\contentsline {subparagraph}{\nonumberline \emph  {Disadvantages:}}{4}{subparagraph*.30}\protected@file@percent }
\newlabel{disadvantages}{{}{4}{\texorpdfstring {\emph {Disadvantages:}}{Disadvantages:}}{subparagraph*.30}{}}
\@writefile{toc}{\contentsline {subparagraph}{\nonumberline \emph  {Real-world applications:}}{4}{subparagraph*.32}\protected@file@percent }
\newlabel{real-world-applications}{{}{4}{\texorpdfstring {\emph {Real-world applications:}}{Real-world applications:}}{subparagraph*.32}{}}
\@writefile{toc}{\contentsline {subsection}{\nonumberline Here are the equations and explanations for KNN Classification and Regression:}{4}{subsection*.34}\protected@file@percent }
\newlabel{here-are-the-equations-and-explanations-for-knn-classification-and-regression}{{}{4}{Here are the equations and explanations for KNN Classification and Regression:}{subsection*.34}{}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline \emph  {KNN Classification}}{4}{subsubsection*.36}\protected@file@percent }
\newlabel{knn-classification-1}{{}{4}{\texorpdfstring {\emph {KNN Classification}}{KNN Classification}}{subsubsection*.36}{}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline \emph  {Majority Voting}}{4}{paragraph*.38}\protected@file@percent }
\newlabel{majority-voting}{{}{4}{\texorpdfstring {\emph {Majority Voting}}{Majority Voting}}{paragraph*.38}{}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline \emph  {Weighted Voting}}{5}{paragraph*.40}\protected@file@percent }
\newlabel{weighted-voting}{{}{5}{\texorpdfstring {\emph {Weighted Voting}}{Weighted Voting}}{paragraph*.40}{}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline \emph  {KNN Regression}}{5}{subsubsection*.42}\protected@file@percent }
\newlabel{knn-regression-2}{{}{5}{\texorpdfstring {\emph {KNN Regression}}{KNN Regression}}{subsubsection*.42}{}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline \emph  {Simple Average}}{5}{paragraph*.44}\protected@file@percent }
\newlabel{simple-average}{{}{5}{\texorpdfstring {\emph {Simple Average}}{Simple Average}}{paragraph*.44}{}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline \emph  {Weighted Average}}{5}{paragraph*.46}\protected@file@percent }
\newlabel{weighted-average}{{}{5}{\texorpdfstring {\emph {Weighted Average}}{Weighted Average}}{paragraph*.46}{}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline \emph  {Distance Metrics}}{5}{subsubsection*.48}\protected@file@percent }
\newlabel{distance-metrics}{{}{5}{\texorpdfstring {\emph {Distance Metrics}}{Distance Metrics}}{subsubsection*.48}{}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline \emph  {KNN Algorithm}}{6}{subsubsection*.50}\protected@file@percent }
\newlabel{knn-algorithm}{{}{6}{\texorpdfstring {\emph {KNN Algorithm}}{KNN Algorithm}}{subsubsection*.50}{}}
\@writefile{toc}{\contentsline {subsection}{\nonumberline 3. Suppose we have a data set with five predictors, \$X\_1 = GPA, X\_2 = IQ, X\_3 = Level \$ (1 for College and 0 for High School), \(X_4 = Interaction\) between GPA and IQ, and \(X_5 = Interaction\) between GPA and Level. The response is starting salary after graduation (in thousands of dollars). Suppose we use least squares to fit the model, and get \(\mitbeta _0 = 50, \mitbeta _1 = 20, \mitbeta _2 = 0.07, \mitbeta _3 = 35, \mitbeta _4 = 0.01, \mitbeta _5 = −10\).}{6}{subsection*.52}\protected@file@percent }
\newlabel{suppose-we-have-a-data-set-with-five-predictors-x_1-gpa-x_2-iq-x_3-level-1-for-college-and-0-for-high-school-x_4-interaction-between-gpa-and-iq-and-x_5-interaction-between-gpa-and-level.-the-response-is-starting-salary-after-graduation-in-thousands-of-dollars.-suppose-we-use-least-squares-to-fit-the-model-and-get-beta_0-50-beta_1-20-beta_2-0.07-beta_3-35-beta_4-0.01-beta_5-10.}{{}{6}{\texorpdfstring {3. Suppose we have a data set with five predictors, \$X\_1 = GPA, X\_2 = IQ, X\_3 = Level \$ (1 for College and 0 for High School), \(X_4 = Interaction\) between GPA and IQ, and \(X_5 = Interaction\) between GPA and Level. The response is starting salary after graduation (in thousands of dollars). Suppose we use least squares to fit the model, and get \(\beta _0 = 50, \beta _1 = 20, \beta _2 = 0.07, \beta _3 = 35, \beta _4 = 0.01, \beta _5 = −10\).}{3. Suppose we have a data set with five predictors, \$X\_1 = GPA, X\_2 = IQ, X\_3 = Level \$ (1 for College and 0 for High School), X\_4 = Interaction between GPA and IQ, and X\_5 = Interaction between GPA and Level. The response is starting salary after graduation (in thousands of dollars). Suppose we use least squares to fit the model, and get \textbackslash beta\_0 = 50, \textbackslash beta\_1 = 20, \textbackslash beta\_2 = 0.07, \textbackslash beta\_3 = 35, \textbackslash beta\_4 = 0.01, \textbackslash beta\_5 = −10.}}{subsection*.52}{}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline (a) Which answer is correct, and why?}{6}{subsubsection*.54}\protected@file@percent }
\newlabel{a-which-answer-is-correct-and-why}{{}{6}{(a) Which answer is correct, and why?}{subsubsection*.54}{}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline i. For a fixed value of IQ and GPA, high school graduates earn more, on average, than college graduates.}{6}{paragraph*.56}\protected@file@percent }
\newlabel{i.-for-a-fixed-value-of-iq-and-gpa-high-school-graduates-earn-more-on-average-than-college-graduates.}{{}{6}{i. For a fixed value of IQ and GPA, high school graduates earn more, on average, than college graduates}{paragraph*.56}{}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline ii. For a fixed value of IQ and GPA, college graduates earn more, on average, than high school graduates.}{6}{paragraph*.58}\protected@file@percent }
\newlabel{ii.-for-a-fixed-value-of-iq-and-gpa-college-graduates-earn-more-on-average-than-high-school-graduates.}{{}{6}{ii. For a fixed value of IQ and GPA, college graduates earn more, on average, than high school graduates}{paragraph*.58}{}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline iii. For a fixed value of IQ and GPA, high school graduates earn more, on average, than college graduates provided that the GPA is high enough.}{6}{paragraph*.60}\protected@file@percent }
\newlabel{iii.-for-a-fixed-value-of-iq-and-gpa-high-school-graduates-earn-more-on-average-than-college-graduates-provided-that-the-gpa-is-high-enough.}{{}{6}{iii. For a fixed value of IQ and GPA, high school graduates earn more, on average, than college graduates provided that the GPA is high enough}{paragraph*.60}{}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline iv. For a fixed value of IQ and GPA, college graduates earn more, on average, than high school graduates provided that the GPA is high enough.}{6}{paragraph*.62}\protected@file@percent }
\newlabel{iv.-for-a-fixed-value-of-iq-and-gpa-college-graduates-earn-more-on-average-than-high-school-graduates-provided-that-the-gpa-is-high-enough.}{{}{6}{iv. For a fixed value of IQ and GPA, college graduates earn more, on average, than high school graduates provided that the GPA is high enough}{paragraph*.62}{}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline (b) Predict the salary of a college graduate with IQ of 110 and a GPA of 4.0.}{7}{subsubsection*.64}\protected@file@percent }
\newlabel{b-predict-the-salary-of-a-college-graduate-with-iq-of-110-and-a-gpa-of-4.0.}{{}{7}{(b) Predict the salary of a college graduate with IQ of 110 and a GPA of 4.0}{subsubsection*.64}{}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline (c) True or false: Since the coefficient for the GPA/IQ interaction term is very small, there is very little evidence of an interaction effect. Justify your answer.}{7}{subsubsection*.66}\protected@file@percent }
\newlabel{c-true-or-false-since-the-coefficient-for-the-gpaiq-interaction-term-is-very-small-there-is-very-little-evidence-of-an-interaction-effect.-justify-your-answer.}{{}{7}{(c) True or false: Since the coefficient for the GPA/IQ interaction term is very small, there is very little evidence of an interaction effect. Justify your answer}{subsubsection*.66}{}}
\@writefile{toc}{\contentsline {subsection}{\nonumberline 4. I collect a set of data (n = 100 observations) containing a single predictor and a quantitative response. I then fit a linear regression model to the data, as well as a separate cubic regression, i.e.~\(Y = \mitbeta _0 + \mitbeta _1 X + \mitbeta _2 X^2 + \mitbeta _3 X^3 + \mitepsilon \).}{8}{subsection*.68}\protected@file@percent }
\newlabel{i-collect-a-set-of-data-n-100-observations-containing-a-single-predictor-and-a-quantitative-response.-i-then-fit-a-linear-regression-model-to-the-data-as-well-as-a-separate-cubic-regression-i.e.-y-beta_0-beta_1-x-beta_2-x2-beta_3-x3-epsilon.}{{}{8}{\texorpdfstring {4. I collect a set of data (n = 100 observations) containing a single predictor and a quantitative response. I then fit a linear regression model to the data, as well as a separate cubic regression, i.e.~\(Y = \beta _0 + \beta _1 X + \beta _2 X^2 + \beta _3 X^3 + \epsilon \).}{4. I collect a set of data (n = 100 observations) containing a single predictor and a quantitative response. I then fit a linear regression model to the data, as well as a separate cubic regression, i.e.~Y = \textbackslash beta\_0 + \textbackslash beta\_1 X + \textbackslash beta\_2 X\^{}2 + \textbackslash beta\_3 X\^{}3 + \textbackslash epsilon.}}{subsection*.68}{}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline (a) Suppose that the true relationship between X and Y is linear, i.e.~\(Y = \mitbeta _0 + \mitbeta _1 X + \mitepsilon \). Consider the training residual sum of squares (RSS) for the linear regression, and also the training RSS for the cubic regression. Would we expect one to be lower than the other, would we expect them to be the same, or is there not enough information to tell? Justify your answer.}{8}{subsubsection*.70}\protected@file@percent }
\newlabel{a-suppose-that-the-true-relationship-between-x-and-y-is-linear-i.e.-y-beta_0-beta_1-x-epsilon.-consider-the-training-residual-sum-of-squares-rss-for-the-linear-regression-and-also-the-training-rss-for-the-cubic-regression.-would-we-expect-one-to-be-lower-than-the-other-would-we-expect-them-to-be-the-same-or-is-there-not-enough-information-to-tell-justify-your-answer.}{{}{8}{\texorpdfstring {(a) Suppose that the true relationship between X and Y is linear, i.e.~\(Y = \beta _0 + \beta _1 X + \epsilon \). Consider the training residual sum of squares (RSS) for the linear regression, and also the training RSS for the cubic regression. Would we expect one to be lower than the other, would we expect them to be the same, or is there not enough information to tell? Justify your answer.}{(a) Suppose that the true relationship between X and Y is linear, i.e.~Y = \textbackslash beta\_0 + \textbackslash beta\_1 X + \textbackslash epsilon. Consider the training residual sum of squares (RSS) for the linear regression, and also the training RSS for the cubic regression. Would we expect one to be lower than the other, would we expect them to be the same, or is there not enough information to tell? Justify your answer.}}{subsubsection*.70}{}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline (b) Answer (a) using test rather than training RSS.}{11}{subsubsection*.72}\protected@file@percent }
\newlabel{b-answer-a-using-test-rather-than-training-rss.}{{}{11}{(b) Answer (a) using test rather than training RSS}{subsubsection*.72}{}}
\gdef \LT@ii {\LT@entry 
    {3}{36.60526pt}\LT@entry 
    {1}{61.7787pt}\LT@entry 
    {1}{50.31465pt}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline However, since we cannot see this clearly for a single regression model, let's run a simulation of 100 iterations of the regressions and check our results.}{13}{subsubsection*.74}\protected@file@percent }
\newlabel{however-since-we-cannot-see-this-clearly-for-a-single-regression-model-lets-run-a-simulation-of-100-iterations-of-the-regressions-and-check-our-results.}{{}{13}{However, since we cannot see this clearly for a single regression model, let's run a simulation of 100 iterations of the regressions and check our results}{subsubsection*.74}{}}
\gdef \LT@iii {\LT@entry 
    {3}{11.47502pt}\LT@entry 
    {1}{134.66188pt}\LT@entry 
    {1}{129.19783pt}\LT@entry 
    {1}{132.04483pt}\LT@entry 
    {1}{126.58078pt}\LT@entry 
    {1}{202.6723pt}\LT@entry 
    {1}{148.17418pt}\LT@entry 
    {1}{200.05525pt}\LT@entry 
    {1}{187.10141pt}\LT@entry 
    {1}{165.67227pt}\LT@entry 
    {1}{145.55713pt}\LT@entry 
    {1}{151.10878pt}\LT@entry 
    {1}{142.49173pt}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline This still isn't conclusive enough. So we run multiple simulations of different linear models.}{15}{subsubsection*.76}\protected@file@percent }
\newlabel{this-still-isnt-conclusive-enough.-so-we-run-multiple-simulations-of-different-linear-models.}{{}{15}{This still isn't conclusive enough. So we run multiple simulations of different linear models}{subsubsection*.76}{}}
\gdef \LT@iv {\LT@entry 
    {3}{11.47502pt}\LT@entry 
    {1}{134.66188pt}\LT@entry 
    {1}{129.19783pt}\LT@entry 
    {1}{132.04483pt}\LT@entry 
    {1}{126.58078pt}\LT@entry 
    {1}{202.6723pt}\LT@entry 
    {1}{148.17418pt}\LT@entry 
    {1}{200.05525pt}\LT@entry 
    {1}{187.10141pt}\LT@entry 
    {1}{165.67227pt}\LT@entry 
    {1}{145.55713pt}\LT@entry 
    {1}{151.10878pt}\LT@entry 
    {1}{142.49173pt}}
\gdef \LT@v {\LT@entry 
    {3}{11.47502pt}\LT@entry 
    {1}{134.66188pt}\LT@entry 
    {1}{129.19783pt}\LT@entry 
    {1}{132.04483pt}\LT@entry 
    {1}{126.58078pt}\LT@entry 
    {1}{202.6723pt}\LT@entry 
    {1}{148.17418pt}\LT@entry 
    {1}{200.05525pt}\LT@entry 
    {1}{187.10141pt}\LT@entry 
    {1}{165.67227pt}\LT@entry 
    {1}{145.55713pt}\LT@entry 
    {1}{151.10878pt}\LT@entry 
    {1}{142.49173pt}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline (c) Suppose that the true relationship between X and Y is not linear, but we don't know how far it is from linear. Consider the training RSS for cubic regression. Would we expect one to be lower than the linear regression, and also the training RSS for the cubic regression. Would we expect one to be lower than the other, would we expect them to be the same, or is there not enough information to tell? Justify your answer.}{16}{subsubsection*.78}\protected@file@percent }
\newlabel{c-suppose-that-the-true-relationship-between-x-and-y-is-not-linear-but-we-dont-know-how-far-it-is-from-linear.-consider-the-training-rss-for-cubic-regression.-would-we-expect-one-to-be-lower-than-the-linear-regression-and-also-the-training-rss-for-the-cubic-regression.-would-we-expect-one-to-be-lower-than-the-other-would-we-expect-them-to-be-the-same-or-is-there-not-enough-information-to-tell-justify-your-answer.}{{}{16}{(c) Suppose that the true relationship between X and Y is not linear, but we don't know how far it is from linear. Consider the training RSS for cubic regression. Would we expect one to be lower than the linear regression, and also the training RSS for the cubic regression. Would we expect one to be lower than the other, would we expect them to be the same, or is there not enough information to tell? Justify your answer}{subsubsection*.78}{}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline (d) Answer (c) using test rather than training RSS.}{16}{subsubsection*.80}\protected@file@percent }
\newlabel{d-answer-c-using-test-rather-than-training-rss.}{{}{16}{(d) Answer (c) using test rather than training RSS}{subsubsection*.80}{}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline For Slightly non-linear equation}{16}{subsubsection*.82}\protected@file@percent }
\newlabel{for-slightly-non-linear-equation}{{}{16}{For Slightly non-linear equation}{subsubsection*.82}{}}
\gdef \LT@vi {\LT@entry 
    {3}{11.47502pt}\LT@entry 
    {1}{134.66188pt}\LT@entry 
    {1}{129.19783pt}\LT@entry 
    {1}{132.04483pt}\LT@entry 
    {1}{126.58078pt}\LT@entry 
    {1}{202.6723pt}\LT@entry 
    {1}{148.17418pt}\LT@entry 
    {1}{200.05525pt}\LT@entry 
    {1}{187.10141pt}\LT@entry 
    {1}{165.67227pt}\LT@entry 
    {1}{145.55713pt}\LT@entry 
    {1}{151.10878pt}\LT@entry 
    {1}{142.49173pt}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Mostly non-linear equation}{17}{subsubsection*.84}\protected@file@percent }
\newlabel{mostly-non-linear-equation}{{}{17}{Mostly non-linear equation}{subsubsection*.84}{}}
\@writefile{toc}{\contentsline {subsection}{\nonumberline 5. Consider the fitted values that result from performing linear regression without an intercept. In this setting, the \(i_{th}\) fitted value takes the form}{17}{subsection*.86}\protected@file@percent }
\newlabel{consider-the-fitted-values-that-result-from-performing-linear-regression-without-an-intercept.-in-this-setting-the-i_th-fitted-value-takes-the-form}{{}{17}{\texorpdfstring {5. Consider the fitted values that result from performing linear regression without an intercept. In this setting, the \(i_{th}\) fitted value takes the form}{5. Consider the fitted values that result from performing linear regression without an intercept. In this setting, the i\_\{th\} fitted value takes the form}}{subsection*.86}{}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Show that we can write}{17}{subsubsection*.88}\protected@file@percent }
\newlabel{show-that-we-can-write}{{}{17}{Show that we can write}{subsubsection*.88}{}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline What is \(a_{i^{'}}\)?}{19}{subsubsection*.90}\protected@file@percent }
\newlabel{what-is-a_i}{{}{19}{\texorpdfstring {What is \(a_{i^{'}}\)?}{What is a\_\{i\^{}\{\textquotesingle \}\}?}}{subsubsection*.90}{}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Note: We interpret this result by saying that the fitted values from linear regression are linear combinations of the response values.}{19}{subsubsection*.92}\protected@file@percent }
\newlabel{note-we-interpret-this-result-by-saying-that-the-fitted-values-from-linear-regression-are-linear-combinations-of-the-response-values.}{{}{19}{Note: We interpret this result by saying that the fitted values from linear regression are linear combinations of the response values}{subsubsection*.92}{}}
\@writefile{toc}{\contentsline {subsection}{\nonumberline 6. Using (3.4), argue that in the case of simple linear regression, the least squares line always passes through the point \((\bar {x}, \bar {y})\).}{21}{subsection*.94}\protected@file@percent }
\newlabel{using-3.4-argue-that-in-the-case-of-simple-linear-regression-the-least-squares-line-always-passes-through-the-point-barx-bary.}{{}{21}{\texorpdfstring {6. Using (3.4), argue that in the case of simple linear regression, the least squares line always passes through the point \((\bar {x}, \bar {y})\).}{6. Using (3.4), argue that in the case of simple linear regression, the least squares line always passes through the point (\textbackslash bar\{x\}, \textbackslash bar\{y\}).}}{subsection*.94}{}}
\@writefile{toc}{\contentsline {subsection}{\nonumberline 7. It is claimed in the text that in the case of simple linear regression of Y onto X, the \(R^2\) statistic (3.17) is equal to the square of the correlation between X and Y (3.18). Prove that this is the case. For simplicity, you may assume that \(\bar {x} = \bar {y} = 0\).}{23}{subsection*.96}\protected@file@percent }
\newlabel{it-is-claimed-in-the-text-that-in-the-case-of-simple-linear-regression-of-y-onto-x-the-r2-statistic-3.17-is-equal-to-the-square-of-the-correlation-between-x-and-y-3.18.-prove-that-this-is-the-case.-for-simplicity-you-may-assume-that-barx-bary-0.}{{}{23}{\texorpdfstring {7. It is claimed in the text that in the case of simple linear regression of Y onto X, the \(R^2\) statistic (3.17) is equal to the square of the correlation between X and Y (3.18). Prove that this is the case. For simplicity, you may assume that \(\bar {x} = \bar {y} = 0\).}{7. It is claimed in the text that in the case of simple linear regression of Y onto X, the R\^{}2 statistic (3.17) is equal to the square of the correlation between X and Y (3.18). Prove that this is the case. For simplicity, you may assume that \textbackslash bar\{x\} = \textbackslash bar\{y\} = 0.}}{subsection*.96}{}}
